{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Friends with Kubernetes HackerNews comments on any Kubernetes-related article tend to fall in three categories: People that did not bump into the problems that Kubernetes solves and consider Kubernetes an unnecessary complication. (This is a very fair point!) People that did bump into the problems that Kubernetes solves and found it an invaluable tool. People that did bump into the problems that Kubernetes solves, but didn't manage to effectively use Kubernetes to solve said problems. This tutorial is for the third category. Goals After completing this tutorial: You will understand common pitfalls when porting an application to Kubernetes. You will know how to re-engineer an application to run on Kubernetes. Non-Goals Teaching Docker/Dockerfile/containerization basics: Please head to Docker Get Started . Teaching Kubernetes basics: Please head to Kubernetes Tutorials . Teaching Helm basics: Please head to Helm Tutorial . Preparations You will need basic shell scripting tools, such as bash , curl , git and make . Here is how you install them in Ubuntu: sudo apt-get install bash curl git make Furthermore, you will need: docker helm kubectl minikube We will use Minikube to provision a small, local Kubernetes cluster. Start a Minikube cluster as follows: minikube start We will need the Ingress addons: minikube addons enable ingress Check that everything works, as follows: kubectl get -n kube-system deploy ingress-nginx-controller > /dev/null && echo \"==> kubectl and minikube work\" eval $(minikube docker-env) docker ps > /dev/null && echo \"==> minikube's docker works\" helm version > /dev/null && echo \"==> helm works\" Click Next below to get started!","title":"Home"},{"location":"#friends-with-kubernetes","text":"HackerNews comments on any Kubernetes-related article tend to fall in three categories: People that did not bump into the problems that Kubernetes solves and consider Kubernetes an unnecessary complication. (This is a very fair point!) People that did bump into the problems that Kubernetes solves and found it an invaluable tool. People that did bump into the problems that Kubernetes solves, but didn't manage to effectively use Kubernetes to solve said problems. This tutorial is for the third category.","title":"Friends with Kubernetes"},{"location":"#goals","text":"After completing this tutorial: You will understand common pitfalls when porting an application to Kubernetes. You will know how to re-engineer an application to run on Kubernetes.","title":"Goals"},{"location":"#non-goals","text":"Teaching Docker/Dockerfile/containerization basics: Please head to Docker Get Started . Teaching Kubernetes basics: Please head to Kubernetes Tutorials . Teaching Helm basics: Please head to Helm Tutorial .","title":"Non-Goals"},{"location":"#preparations","text":"You will need basic shell scripting tools, such as bash , curl , git and make . Here is how you install them in Ubuntu: sudo apt-get install bash curl git make Furthermore, you will need: docker helm kubectl minikube We will use Minikube to provision a small, local Kubernetes cluster. Start a Minikube cluster as follows: minikube start We will need the Ingress addons: minikube addons enable ingress Check that everything works, as follows: kubectl get -n kube-system deploy ingress-nginx-controller > /dev/null && echo \"==> kubectl and minikube work\" eval $(minikube docker-env) docker ps > /dev/null && echo \"==> minikube's docker works\" helm version > /dev/null && echo \"==> helm works\" Click Next below to get started!","title":"Preparations"},{"location":"state-creep/","text":"In this part, we will show via a hands-on example a common problem with porting application to Kubernetes: state creep . Although a service may seem stateless, large legacy codes hidden behind layers of libraries leads to state \"creeping\" unexpectedly. We start by presenting such a service with state creep, what goes wrong when deploying it on top of Kubernetes and how to fix it. Illustrative Example Imagine the following situation. You are working in a regulated environment, such as healthcare . You want to make sure that your users' passwords are safe, hence you use Secure Remote Password protocol (SRP) for authentication. In brief, SRP does not require the client to send the password to the server, nor the server to store the password. Instead, a sophisticated exchange allows the client to prove to the server that it knows the password. Similarly, the client can verify that the server knows the password. For now, that is all you need to know about SRP. Running on Kubernetes The SRP server of your company was coded years ago. Let us run it on top of Kubernetes: git clone https://github.com/elastisys/tutorial-friends-with-kubernetes cd tutorial-friends-with-kubernetes/code Note To simplify this tutorial, the srp-server includes a hard-coded database with a single test user. Thanks to the magic of ready-made tutorials, the Dockerfile and Kubernetes resources are already written. Dockerfile FROM golang:alpine as builder WORKDIR /go/src/app COPY . . RUN CGO_ENABLED = 0 GOOS = linux go build -a -o /main -mod = vendor FROM golang:latest COPY --from = builder /main /main RUN ls -l / ENTRYPOINT [ \"/main\" ] srp-server-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : labels : app : srp-server name : srp-server spec : replicas : 1 selector : matchLabels : app : srp-server template : metadata : labels : app : srp-server spec : containers : - image : srp-server:latest imagePullPolicy : Never name : srp-server srp-server-service.yaml apiVersion : v1 kind : Service metadata : labels : app : srp-server name : srp-server spec : ports : - name : http port : 8080 selector : app : srp-server srp-server-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : srp-server labels : app : srp-server spec : rules : - http : paths : - path : /auth pathType : Prefix backend : serviceName : srp-server servicePort : 8080 If you are familiar with Go applications, then the Dockerfile should be straight forward. Otherwise, you may want to read how to containerize Go applications . For running the application in Kubernetes, we need three concepts. The Deployment ensures that our code is running in the cluster, i.e., that at least one Pod is running. The Service points to the running Pods, so they can be found inside the cluster. Finally, the Ingress allows traffic to flow from outside the cluster to the Service inside the cluster. Now let's build the container image: Note To simplify this tutorial, you will build the container image directly inside the Docker Daemon of Minikube. Usually, you should push container images to a registry. eval $( minikube docker-env ) docker build -t srp-server srp-server And deploy the application in the Kubernetes cluster: kubectl apply -f srp-server/deploy Looks good. Now let's check if the Pods are comming up: kubectl get pods You should see something like this: NAME READY STATUS RESTARTS AGE srp-server-7f7dfc86fd-tt2rv 1/1 Running 0 5s Awesome! The SRP server seems to work. That was really easy. Let's check if it can actually perform logins. To this end, we will build srp-client and use kubectl run to run it interactively inside the cluster. eval $( minikube docker-env ) docker build -t srp-client srp-client kubectl run \\ -ti \\ --rm \\ --generator = run-pod/v1 \\ --image srp-client \\ --image-pull-policy Never \\ srp-client \\ http:// $( minikube ip ) You should see a screen full of green checkboxes, as shown in the screenshot below. Great success! Trouble Ahead: Scaling Does Not Work The world is now a different place and your healthcare application is getting popular. Time to scale the SRP server up. Kubernetes should make this straight-forward thanks to the kubectl scale command. Leave srp-client running and in a new terminal type: kubectl scale --replicas=2 deployment/srp-server And, as you wait for srp-server to scale up ... ... Auch! That does not look too good! Authentication failures are littering your terminal. Let's check if there is something wrong with Kubernetes: kubectl get pods You should see something like this: NAME READY STATUS RESTARTS AGE srp-server-7f7dfc86fd-tsxjp 1/1 Running 0 3s srp-server-7f7dfc86fd-tt2rv 1/1 Running 0 8m38s Both Pods srp-server seem fine. Their status is Running , they feature no restarts. Let's check if the application shows some suspicious logs: Note We can use -l app=srp-server to express interest in all Pods of srp-server . This is because the Deployment includes the following snippet: metadata: labels: app: srp-server kubectl logs -l app = srp-server You should see something like this: 2020/08/28 09:04:02 Challenge sent for \"test@example.com\" 2020/08/28 09:04:01 Error: \"No authentication session found\" 2020/08/28 09:04:01 Challenge sent for \"test@example.com\" 2020/08/28 09:04:01 Error: \"Invalid username or password\" 2020/08/28 09:04:01 Error: \"No authentication session found\" Okey, so client requests obviously arrive at srp-service but the logs are littered with errors! The users are obviously impacted, so let's scale the application back down: kubectl scale --replicas=1 deployment/srp-server Fortunately, the errors are gone now. However, what remains is the sour aftertaste of Kubernetes failing its promise to facilitate scalability. What Happened? It sadly became obvious that we cannot simply deploy the application without understanding how it works. Let us look closer at SRP. The sequence diagram from simbo1905 explains it best: In essence, SRP is composed of two requests, challenge and authenticate, initiated by the client. It almost looks like the flow is stateless, except for one item that stands out: the challenge cache! Aha! The server needs to store some state between issuing a challenge and authenticating the client. It cannot trust the client to store this information, as this would void the security guarantees of SRP. But wait! If srp-server is scaled to two replicas, what happens to the challenge cache? A quick inspection of code/srp-server/srp-server.go reveals that the challenge cache is local to each replica: var authSessionCache = map[string](*srp.SRPServer){} As Kubernetes tries to balance the load across replicas, the likelihood of the client getting the challenge from one replica and authenticating against a different replica is high. Nobody makes such an obvious mistake! You might argue that this is a constructed problem, a mistake we sneaked in just to give purpose to this tutorial. And, of course, this is a minimal not-so-working example, so it may feel a bit artificial. But trust me! Our experience shows that state creep is a real issue and getting it right is key to successful cloud-native application design. As legacy code is exposed to new situations, hidden behind layers and layers of libraries, state creep is a real barrier to Kubernetes adoption. There are several solutions to this problem: Push the state to the client: Given the nature of SRP, you would need to use authenticated encryption for that. The downside is that you need to change the client-server API. Sticky sessions or static source-IP load-balancing: This ensures that the client asks for a challenge and authenticates against the same replica. This is a quick fix, but will brings more issues down the road with scaling down, rolling updates, etc. Share the challenge cache: This ensures that each replica has access to the same challenge cache. Let's go for the last solution. It requires no API changes, and will prepare us for scaling down and rolling updates. Moving the Challenge Cache to Redis Redis is a popular project in the cloud native ecosystem to store short-lived (\"cache\") state. While pushing state out of your service into ... another service may feel like cheating, Redis is well equipped to handle state: It supports multiple replicas, with proper state replication and fail-over. Of course, your service could implement such state handling too, but by the time you are done you essentially have an ad hoc, informally-specified, bug-ridden, slow implementation of half of Redis . ( Greenspun's tenth rule for cloud native software?) Assuming I convinced you, let's spin up a Redis cluster: helm repo add bitnami https://charts.bitnami.com/bitnami helm install redis bitnami/redis Now let's change the application to store the challenge cache in Redis. Thanks to the magic of tutorials, the source code is already available in srp-server-redis . I here assume that you are able to read Go code, although you are not required to be a Go programmer to understand this part. I suggest you look at the changes using side-by-side diff: diff --exclude 'go.*' -ru srp-server srp-server-redis | less -S Selected output of diff diff of srp-server.go @@ -12 +17 @@ -var authSessionCache = map[string](*srp.SRPServer){} +var rdb *redis.Client @@ -63 +68,11 @@ func handleChallenge(w http.ResponseWrit - authSessionCache[username] = server + data, err := json.Marshal(&server) + if err != nil { + handleError(w, http.StatusInternalServerError, fmt.Sprintf(\"%v\", err)) + return + } + + err = rdb.Set(r.Context(), username, data, time.Minute).Err() + if err != nil { + handleError(w, http.StatusInternalServerError, fmt.Sprintf(\"%v\", err)) + return + } @@ -78,4 +93,10 @@ func handleAuthentication(w http.Respons - server, ok := authSessionCache[areq.Username] - defer delete(authSessionCache, areq.Username) - if !ok { - handleError(w, http.StatusBadRequest, \"No authentication session found\") + data, err := rdb.Get(r.Context(), areq.Username).Bytes() + if err != nil { + handleError(w, http.StatusBadRequest, fmt.Sprintf(\"No authentication session found: %v\", err)) + return + } + + server := srp.SRPServer{} + err = json.Unmarshal(data, &server) + if err != nil { + handleError(w, http.StatusInternalServerError, fmt.Sprintf(\"Couldn't unmarshall authentication session: %v\", err)) @@ -88 +109 @@ func handleAuthentication(w http.Respons - handleError(w, http.StatusUnauthorized, \"Invalid username or password\") + handleError(w, http.StatusUnauthorized, fmt.Sprintf(\"Invalid username or password: %v\", err)) @@ -97,0 +119,11 @@ func main() { + + rdb = redis.NewClient(&redis.Options{ + Addr: os.Getenv(\"REDIS_MASTER\"), + Password: os.Getenv(\"REDIS_PASSWORD\"), + DB: 0, // use default DB + }) + + pong, err := rdb.Ping(context.Background()).Result() + if err != nil { + log.Fatalln(pong, err) + } diff of deploy/srp-server-deployment.yaml @@ -18 +18 @@ - - image: srp-server:latest + - image: srp-server-redis:latest @@ -20,0 +21,8 @@ + env: + - name: REDIS_MASTER + value: \"redis-master:6379\" + - name: REDIS_PASSWORD + valueFrom: + secretKeyRef: + name: redis + key: \"redis-password\" Let us here briefly discuss the main changes. First, we replaced the authSessionCache global variable with a Redis client (see srp-server.go ), which we use to set and get challenge caches. We took the opportunity to set an expire of 1 minute to each entry, something that the previous code didn't have. (How come the old srp-server didn't crash due to memory exhaution until now?) Second, we changed the Deployment to expose the Redis password (i.e., a Secret ) and the Redis server address to srp-server-redis via environment variables. This is a very common pattern for configuring applications hosted in Kubernetes. So, does it work? eval $( minikube docker-env ) docker build -t srp-server-redis srp-server-redis kubectl apply -f srp-server-redis/deploy kubectl get pods Look at the terminal where the client is running. You should see all green with a single srp-server-redis replica, but did we solve the original problem of scaling up? kubectl scale --replicas = 3 deployment/srp-server Let's watch the replicas coming up: kubectl get pods Now look at the client terminal. There should be zero impact on your client requests and you should see all green. Great! We now have a service that we can properly scale up with zero downtime. Takeaways Kubernetes promises to solve issues, such as scalability, fault-tolerance and zero-downtime updates. To solve these issues, Kubernetes has certain expectations from the hosted application. One of these expectations is for the application to be stateless. State must be stored in services that can handle state with care. As legacy code is reused in new situations, state may creep into what we may believe is a stateless application. Redis is a popular project to store short-lived \"cache\" state. But how do I detect state creep before it's too late? Practice the rule of two. Always have at least two replicas of every code.","title":"State Creep"},{"location":"state-creep/#illustrative-example","text":"Imagine the following situation. You are working in a regulated environment, such as healthcare . You want to make sure that your users' passwords are safe, hence you use Secure Remote Password protocol (SRP) for authentication. In brief, SRP does not require the client to send the password to the server, nor the server to store the password. Instead, a sophisticated exchange allows the client to prove to the server that it knows the password. Similarly, the client can verify that the server knows the password. For now, that is all you need to know about SRP.","title":"Illustrative Example"},{"location":"state-creep/#running-on-kubernetes","text":"The SRP server of your company was coded years ago. Let us run it on top of Kubernetes: git clone https://github.com/elastisys/tutorial-friends-with-kubernetes cd tutorial-friends-with-kubernetes/code Note To simplify this tutorial, the srp-server includes a hard-coded database with a single test user. Thanks to the magic of ready-made tutorials, the Dockerfile and Kubernetes resources are already written. Dockerfile FROM golang:alpine as builder WORKDIR /go/src/app COPY . . RUN CGO_ENABLED = 0 GOOS = linux go build -a -o /main -mod = vendor FROM golang:latest COPY --from = builder /main /main RUN ls -l / ENTRYPOINT [ \"/main\" ] srp-server-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : labels : app : srp-server name : srp-server spec : replicas : 1 selector : matchLabels : app : srp-server template : metadata : labels : app : srp-server spec : containers : - image : srp-server:latest imagePullPolicy : Never name : srp-server srp-server-service.yaml apiVersion : v1 kind : Service metadata : labels : app : srp-server name : srp-server spec : ports : - name : http port : 8080 selector : app : srp-server srp-server-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : srp-server labels : app : srp-server spec : rules : - http : paths : - path : /auth pathType : Prefix backend : serviceName : srp-server servicePort : 8080 If you are familiar with Go applications, then the Dockerfile should be straight forward. Otherwise, you may want to read how to containerize Go applications . For running the application in Kubernetes, we need three concepts. The Deployment ensures that our code is running in the cluster, i.e., that at least one Pod is running. The Service points to the running Pods, so they can be found inside the cluster. Finally, the Ingress allows traffic to flow from outside the cluster to the Service inside the cluster. Now let's build the container image: Note To simplify this tutorial, you will build the container image directly inside the Docker Daemon of Minikube. Usually, you should push container images to a registry. eval $( minikube docker-env ) docker build -t srp-server srp-server And deploy the application in the Kubernetes cluster: kubectl apply -f srp-server/deploy Looks good. Now let's check if the Pods are comming up: kubectl get pods You should see something like this: NAME READY STATUS RESTARTS AGE srp-server-7f7dfc86fd-tt2rv 1/1 Running 0 5s Awesome! The SRP server seems to work. That was really easy. Let's check if it can actually perform logins. To this end, we will build srp-client and use kubectl run to run it interactively inside the cluster. eval $( minikube docker-env ) docker build -t srp-client srp-client kubectl run \\ -ti \\ --rm \\ --generator = run-pod/v1 \\ --image srp-client \\ --image-pull-policy Never \\ srp-client \\ http:// $( minikube ip ) You should see a screen full of green checkboxes, as shown in the screenshot below. Great success!","title":"Running on Kubernetes"},{"location":"state-creep/#trouble-ahead-scaling-does-not-work","text":"The world is now a different place and your healthcare application is getting popular. Time to scale the SRP server up. Kubernetes should make this straight-forward thanks to the kubectl scale command. Leave srp-client running and in a new terminal type: kubectl scale --replicas=2 deployment/srp-server And, as you wait for srp-server to scale up ... ... Auch! That does not look too good! Authentication failures are littering your terminal. Let's check if there is something wrong with Kubernetes: kubectl get pods You should see something like this: NAME READY STATUS RESTARTS AGE srp-server-7f7dfc86fd-tsxjp 1/1 Running 0 3s srp-server-7f7dfc86fd-tt2rv 1/1 Running 0 8m38s Both Pods srp-server seem fine. Their status is Running , they feature no restarts. Let's check if the application shows some suspicious logs: Note We can use -l app=srp-server to express interest in all Pods of srp-server . This is because the Deployment includes the following snippet: metadata: labels: app: srp-server kubectl logs -l app = srp-server You should see something like this: 2020/08/28 09:04:02 Challenge sent for \"test@example.com\" 2020/08/28 09:04:01 Error: \"No authentication session found\" 2020/08/28 09:04:01 Challenge sent for \"test@example.com\" 2020/08/28 09:04:01 Error: \"Invalid username or password\" 2020/08/28 09:04:01 Error: \"No authentication session found\" Okey, so client requests obviously arrive at srp-service but the logs are littered with errors! The users are obviously impacted, so let's scale the application back down: kubectl scale --replicas=1 deployment/srp-server Fortunately, the errors are gone now. However, what remains is the sour aftertaste of Kubernetes failing its promise to facilitate scalability.","title":"Trouble Ahead: Scaling Does Not Work"},{"location":"state-creep/#what-happened","text":"It sadly became obvious that we cannot simply deploy the application without understanding how it works. Let us look closer at SRP. The sequence diagram from simbo1905 explains it best: In essence, SRP is composed of two requests, challenge and authenticate, initiated by the client. It almost looks like the flow is stateless, except for one item that stands out: the challenge cache! Aha! The server needs to store some state between issuing a challenge and authenticating the client. It cannot trust the client to store this information, as this would void the security guarantees of SRP. But wait! If srp-server is scaled to two replicas, what happens to the challenge cache? A quick inspection of code/srp-server/srp-server.go reveals that the challenge cache is local to each replica: var authSessionCache = map[string](*srp.SRPServer){} As Kubernetes tries to balance the load across replicas, the likelihood of the client getting the challenge from one replica and authenticating against a different replica is high. Nobody makes such an obvious mistake! You might argue that this is a constructed problem, a mistake we sneaked in just to give purpose to this tutorial. And, of course, this is a minimal not-so-working example, so it may feel a bit artificial. But trust me! Our experience shows that state creep is a real issue and getting it right is key to successful cloud-native application design. As legacy code is exposed to new situations, hidden behind layers and layers of libraries, state creep is a real barrier to Kubernetes adoption. There are several solutions to this problem: Push the state to the client: Given the nature of SRP, you would need to use authenticated encryption for that. The downside is that you need to change the client-server API. Sticky sessions or static source-IP load-balancing: This ensures that the client asks for a challenge and authenticates against the same replica. This is a quick fix, but will brings more issues down the road with scaling down, rolling updates, etc. Share the challenge cache: This ensures that each replica has access to the same challenge cache. Let's go for the last solution. It requires no API changes, and will prepare us for scaling down and rolling updates.","title":"What Happened?"},{"location":"state-creep/#moving-the-challenge-cache-to-redis","text":"Redis is a popular project in the cloud native ecosystem to store short-lived (\"cache\") state. While pushing state out of your service into ... another service may feel like cheating, Redis is well equipped to handle state: It supports multiple replicas, with proper state replication and fail-over. Of course, your service could implement such state handling too, but by the time you are done you essentially have an ad hoc, informally-specified, bug-ridden, slow implementation of half of Redis . ( Greenspun's tenth rule for cloud native software?) Assuming I convinced you, let's spin up a Redis cluster: helm repo add bitnami https://charts.bitnami.com/bitnami helm install redis bitnami/redis Now let's change the application to store the challenge cache in Redis. Thanks to the magic of tutorials, the source code is already available in srp-server-redis . I here assume that you are able to read Go code, although you are not required to be a Go programmer to understand this part. I suggest you look at the changes using side-by-side diff: diff --exclude 'go.*' -ru srp-server srp-server-redis | less -S Selected output of diff diff of srp-server.go @@ -12 +17 @@ -var authSessionCache = map[string](*srp.SRPServer){} +var rdb *redis.Client @@ -63 +68,11 @@ func handleChallenge(w http.ResponseWrit - authSessionCache[username] = server + data, err := json.Marshal(&server) + if err != nil { + handleError(w, http.StatusInternalServerError, fmt.Sprintf(\"%v\", err)) + return + } + + err = rdb.Set(r.Context(), username, data, time.Minute).Err() + if err != nil { + handleError(w, http.StatusInternalServerError, fmt.Sprintf(\"%v\", err)) + return + } @@ -78,4 +93,10 @@ func handleAuthentication(w http.Respons - server, ok := authSessionCache[areq.Username] - defer delete(authSessionCache, areq.Username) - if !ok { - handleError(w, http.StatusBadRequest, \"No authentication session found\") + data, err := rdb.Get(r.Context(), areq.Username).Bytes() + if err != nil { + handleError(w, http.StatusBadRequest, fmt.Sprintf(\"No authentication session found: %v\", err)) + return + } + + server := srp.SRPServer{} + err = json.Unmarshal(data, &server) + if err != nil { + handleError(w, http.StatusInternalServerError, fmt.Sprintf(\"Couldn't unmarshall authentication session: %v\", err)) @@ -88 +109 @@ func handleAuthentication(w http.Respons - handleError(w, http.StatusUnauthorized, \"Invalid username or password\") + handleError(w, http.StatusUnauthorized, fmt.Sprintf(\"Invalid username or password: %v\", err)) @@ -97,0 +119,11 @@ func main() { + + rdb = redis.NewClient(&redis.Options{ + Addr: os.Getenv(\"REDIS_MASTER\"), + Password: os.Getenv(\"REDIS_PASSWORD\"), + DB: 0, // use default DB + }) + + pong, err := rdb.Ping(context.Background()).Result() + if err != nil { + log.Fatalln(pong, err) + } diff of deploy/srp-server-deployment.yaml @@ -18 +18 @@ - - image: srp-server:latest + - image: srp-server-redis:latest @@ -20,0 +21,8 @@ + env: + - name: REDIS_MASTER + value: \"redis-master:6379\" + - name: REDIS_PASSWORD + valueFrom: + secretKeyRef: + name: redis + key: \"redis-password\" Let us here briefly discuss the main changes. First, we replaced the authSessionCache global variable with a Redis client (see srp-server.go ), which we use to set and get challenge caches. We took the opportunity to set an expire of 1 minute to each entry, something that the previous code didn't have. (How come the old srp-server didn't crash due to memory exhaution until now?) Second, we changed the Deployment to expose the Redis password (i.e., a Secret ) and the Redis server address to srp-server-redis via environment variables. This is a very common pattern for configuring applications hosted in Kubernetes. So, does it work? eval $( minikube docker-env ) docker build -t srp-server-redis srp-server-redis kubectl apply -f srp-server-redis/deploy kubectl get pods Look at the terminal where the client is running. You should see all green with a single srp-server-redis replica, but did we solve the original problem of scaling up? kubectl scale --replicas = 3 deployment/srp-server Let's watch the replicas coming up: kubectl get pods Now look at the client terminal. There should be zero impact on your client requests and you should see all green. Great! We now have a service that we can properly scale up with zero downtime.","title":"Moving the Challenge Cache to Redis"},{"location":"state-creep/#takeaways","text":"Kubernetes promises to solve issues, such as scalability, fault-tolerance and zero-downtime updates. To solve these issues, Kubernetes has certain expectations from the hosted application. One of these expectations is for the application to be stateless. State must be stored in services that can handle state with care. As legacy code is reused in new situations, state may creep into what we may believe is a stateless application. Redis is a popular project to store short-lived \"cache\" state. But how do I detect state creep before it's too late? Practice the rule of two. Always have at least two replicas of every code.","title":"Takeaways"}]}